# WebGPU LLM Chat

ブラウザ完結型のLLMチャットアプリケーション。WebGPUを使って、ユーザーのブラウザ上で軽量なLLMを動作させます。

## 特徴

- **完全ブラウザ完結**: サーバー不要、全ての処理がブラウザ内で完結
- **WebGPU高速化**: GPUアクセラレーションによる高速な推論
- **モデル比較機能**: 複数のモデルを同時に実行して応答を比較
- **ストリーミング応答**: リアルタイムでAIの応答を表示
- **オフライン対応**: 一度モデルをダウンロードすれば、オフラインでも動作

## 対応モデル

- **Phi-3 Mini (4B)**: Microsoftの高性能小型モデル
- **Llama 3.2 (1B)**: Metaの超軽量モデル
- **Llama 3.2 (3B)**: バランスの良いモデル
- **Gemma 2 (2B)**: Googleの軽量モデル

## 必要環境

- **ブラウザ**: Chrome 113+ または Edge 113+
- **WebGPU対応**: ブラウザでWebGPUが有効になっている必要があります
- **推奨RAM**: 8GB以上
- **ストレージ**: 2-4GB（モデルのキャッシュ用）

## セットアップ

### 1. 依存関係のインストール

```bash
npm install
```

### 2. 開発サーバーの起動

```bash
npm run dev
```

ブラウザで `http://localhost:5173` を開きます。

### 3. プロダクションビルド

```bash
npm run build
npm run preview
```

## 使い方

### 基本的な使い方

1. ヘッダーのドロップダウンからモデルを選択
2. 「ロード」ボタンをクリック（初回は数分かかります）
3. ダウンロード完了後、メッセージを入力してチャット開始

### モデル比較機能

1. モデルをロードした状態で「比較モード」ボタンをクリック
2. 比較したい別のモデルを選択（1-4の数字で選択）
3. 両方のモデルが同時に応答を生成し、並べて表示されます
4. 「通常モード」ボタンで比較モードを終了

## 技術スタック

- **WebLLM**: WebGPU対応のLLMランタイム
- **TypeScript**: 型安全な開発
- **Vite**: 高速なビルドツール
- **バニラJS**: フレームワーク不要のシンプルな実装

## モデルのキャッシュ

モデルは初回ダウンロード後、ブラウザのIndexedDBにキャッシュされます。
2回目以降の起動は高速になります。

キャッシュをクリアしたい場合は、ブラウザの開発者ツールから
Application > Storage > IndexedDB を削除してください。

## トラブルシューティング

### WebGPU未対応エラー

Chrome 113+またはEdge 113+を使用してください。
`chrome://flags` で `#enable-unsafe-webgpu` を有効にする必要がある場合があります。

### メモリ不足エラー

より小さいモデル（Llama 3.2 1Bなど）を選択してください。

### ダウンロードが遅い

初回は2-4GBのモデルをダウンロードするため、時間がかかります。
2回目以降はキャッシュから読み込まれるため高速です。

## ライセンス

MIT
